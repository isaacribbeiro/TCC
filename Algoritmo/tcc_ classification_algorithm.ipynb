{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.pt.examples import sentences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#-----------------------------------------------Normalização----------------------------------------------------#\n",
    "\n",
    "def setup_abbr():\n",
    "    file = open(\"dic_portuguese.txt\", encoding='utf-8')\n",
    "    abbr_dict = {}\n",
    "\n",
    "    for line in file:\n",
    "        w = line.split(\";\")\n",
    "        abbr_dict[w[0]] = w[1].replace(\"\\n\", \"\")\n",
    "    file.close()\n",
    "\n",
    "    return abbr_dict\n",
    "\n",
    "def lemmatizer(doc_corrected):\n",
    "    lemma_sentence = []\n",
    "    for token in doc_corrected: \n",
    "        if token.pos_ == 'VERB':\n",
    "            lemma = token.lemma_\n",
    "            lemma_sentence.append(lemma)\n",
    "        else:\n",
    "            lemma_sentence.append(token.text)\n",
    "    lemmatized_sentence = ' '.join(lemma_sentence)\n",
    "    return lemmatized_sentence\n",
    "\n",
    "def remove_stopword(lemmatized_sentence): \n",
    "    stop_words = set(stopwords.words('portuguese')+ \\\n",
    "    [\"{user}\", \"{url}\", \"<br/>\", \"myfitnesspal\", \"sigaa\", \"neste\"]) \n",
    "    stop_words.remove(\"não\")\n",
    "    stop_words.remove(\"sem\")\n",
    "    word_tokens = word_tokenize(lemmatized_sentence)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)\n",
    "    result = ' '.join(filtered_sentence)\n",
    "    return result\n",
    "\n",
    "data = pd.read_csv('prus.csv')\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "cln = []\n",
    "abbr_dict = setup_abbr()\n",
    "\n",
    "for i in range(len(data)):\n",
    "    doc = nlp(data['comment'][i])\n",
    "    doc_lower = doc.text.lower()\n",
    "    doc_punctuation = re.sub('[^a-zãàáâéêíõóôúç \\n]', ' ', doc_lower)\n",
    "    doc_corrected = nlp(\" \".join([abbr_dict.get(w, w) for w in doc_punctuation.split()])) \n",
    "    lemmatized_sentence = lemmatizer(doc_corrected)\n",
    "    result = remove_stopword(lemmatized_sentence)\n",
    "    cln.append(result)\n",
    "clean = pd.DataFrame(data=np.array(cln), columns= ['comment_clean'])\n",
    "\n",
    "#-----------------------------------------------Classificação---------------------------------------------------#\n",
    "\n",
    "def pattern1(pru):\n",
    "    functionality = ''\n",
    "    hypothesis = ''\n",
    "    j = 0\n",
    "    while (j < (len(pru)-3)):\n",
    "        if pru[j].pos_ == 'ADV':\n",
    "            if pru[j+1].pos_ == 'VERB':\n",
    "                if pru[j+2].pos_ == 'VERB' and (pru[j+3].pos_ == 'NOUN' or pru[j+3].pos_ == 'ADJ'):\n",
    "                    func = [pru[j+2].text, pru[j+3].text]\n",
    "                    functionality = ' '.join(func)\n",
    "                    hypothesis = '3'\n",
    "                    return functionality, hypothesis\n",
    "                elif pru[j+2].pos_ == 'NOUN' or pru[j+2].pos_ == 'VERB' or pru[j+2].pos_ == 'PROPN':\n",
    "                    func = [pru[j+1].text, pru[j+2].text]\n",
    "                    functionality = ' '.join(func)\n",
    "                    hypothesis = '1'\n",
    "                    return functionality, hypothesis\n",
    "        j = j + 1\n",
    "    return functionality, hypothesis\n",
    "\n",
    "def pattern2(pru):\n",
    "    functionality = ''\n",
    "    hypothesis = ''\n",
    "    j = 0\n",
    "    while (j < (len(pru)-3)):\n",
    "        if pru[j].pos_ == 'VERB':\n",
    "            if pru[j+1].pos_ == 'VERB' and pru[j+2].pos_ == 'NOUN':\n",
    "                func = [pru[j+1].text, pru[j+2].text]\n",
    "                functionality = ' '.join(func)\n",
    "                hypothesis = '1'\n",
    "                return functionality, hypothesis\n",
    "            elif pru[j+1].pos_ == 'NOUN' and (pru[j+2].pos_ == 'ADJ' or pru[j+2].pos_ == 'NOUN' or pru[j+2].pos_ == 'VERB') and pru[j+3].pos_ == 'NOUN':\n",
    "                func = [pru[j+2].text, pru[j+3].text]\n",
    "                functionality = ' '.join(func)\n",
    "                hypothesis = '1'\n",
    "                return functionality, hypothesis\n",
    "            elif (pru[j+1].pos_ == 'ADV' or pru[j+1].pos_ == 'AUX') and pru[j+2].pos_ == 'VERB' and pru[j+3].pos_ == 'NOUN':\n",
    "                func = [pru[j+2].text, pru[j+3].text]\n",
    "                functionality = ' '.join(func)\n",
    "                hypothesis = '1'\n",
    "                return functionality, hypothesis\n",
    "        j = j + 1\n",
    "    return functionality, hypothesis\n",
    "\n",
    "def pattern3(pru):\n",
    "    functionality = ''\n",
    "    hypothesis = ''\n",
    "    j = 0\n",
    "    while (j < (len(pru)-3)):\n",
    "        if pru[j].pos_ == 'ADJ' and pru[j+1].pos_ == 'VERB' and pru[j+2].pos_ == 'NOUN':\n",
    "            func = [pru[j+1].text, pru[j+2].text]\n",
    "            functionality = ' '.join(func)\n",
    "            hypothesis = '1'\n",
    "            return functionality, hypothesis\n",
    "        j = j + 1\n",
    "    return functionality, hypothesis\n",
    "\n",
    "def hypothesis_2(pru):\n",
    "    j = 0\n",
    "    while (j < (len(pru)-3)):\n",
    "        if pru[j].text == 'não' and pru[j+1].text == 'conseguir':\n",
    "            return 1\n",
    "        else:\n",
    "            j = j + 1\n",
    "    return 0\n",
    "\n",
    "functionalities = []\n",
    "hypotheses = []\n",
    "h2 = 0\n",
    "\n",
    "for i in range(len(cln)):\n",
    "    pru = nlp(clean['comment_clean'][i])\n",
    "    functionality = '' \n",
    "    hypothesis = ''\n",
    "    if len(pru) > 3:\n",
    "        functionality, hypothesis = pattern1(pru) \n",
    "        if len(functionality) == 0:\n",
    "            functionality, hypothesis = pattern2(pru) \n",
    "        if len(functionality) == 0:\n",
    "            functionality, hypothesis = pattern3(pru) \n",
    "        if len(functionality) == 0:\n",
    "            functionality, hypothesis = 'none', 'none'\n",
    "    else:\n",
    "        functionality, hypothesis = 'none', 'none'\n",
    "    h2 = h2 + hypothesis_2(pru)\n",
    "    functionalities.append(functionality)\n",
    "    hypotheses.append(hypothesis)\n",
    "\n",
    "#Hipótese 2 = 264\n",
    "\n",
    "func = pd.DataFrame(data=np.array(functionalities), index= range(len(data)), columns= ['funcionality'])\n",
    "hypo = pd.DataFrame(data=np.array(hypotheses), index= range(len(data)), columns= ['hypothesis'])\n",
    "df = pd.concat([data, func, hypo], axis=1)\n",
    "df.to_excel('automatic_classification.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
