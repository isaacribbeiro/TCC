{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidades de PRUS pertencentes a hipótese 1: 17\n",
      "Quantidades de PRUS pertencentes a hipótese 2: 9\n",
      "Quantidades de PRUS pertencentes a hipótese 3: 8\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.pt.examples import sentences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#-----------------------------------------------Normalização----------------------------------------------------#\n",
    "\n",
    "def setup_abbr():\n",
    "    file = open(\"dic_portuguese.txt\", encoding='utf-8')\n",
    "    abbr_dict = {}\n",
    "\n",
    "    for line in file:\n",
    "        w = line.split(\";\")\n",
    "        abbr_dict[w[0]] = w[1].replace(\"\\n\", \"\")\n",
    "    file.close()\n",
    "\n",
    "    return abbr_dict\n",
    "\n",
    "def lemmatizer(doc_corrected):\n",
    "    lemma_sentence = []\n",
    "    for token in doc_corrected: \n",
    "        if token.pos_ == 'VERB':\n",
    "            lemma = token.lemma_\n",
    "            lemma_sentence.append(lemma)\n",
    "        else:\n",
    "            lemma_sentence.append(token.text)\n",
    "    lemmatized_sentence = ' '.join(lemma_sentence)\n",
    "    return lemmatized_sentence\n",
    "\n",
    "def remove_stopword(lemmatized_sentence): \n",
    "    stop_words = set(stopwords.words('portuguese')+ \\\n",
    "    [\"{user}\", \"{url}\", \"<br/>\", \"myfitnesspal\", \"sigaa\", \"neste\"]) \n",
    "    stop_words.remove(\"não\")\n",
    "    stop_words.remove(\"sem\")\n",
    "    word_tokens = word_tokenize(lemmatized_sentence)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)\n",
    "    result = ' '.join(filtered_sentence)\n",
    "    return result\n",
    "\n",
    "data = pd.read_csv('tst.csv')\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "cln = []\n",
    "abbr_dict = setup_abbr()\n",
    "\n",
    "for i in range(len(data)):\n",
    "    doc = nlp(data['comments'][i])\n",
    "    doc_lower = doc.text.lower()\n",
    "    doc_punctuation = re.sub('[^a-zãàáâéêíõóôúç \\n]', ' ', doc_lower)\n",
    "    doc_corrected = nlp(\" \".join([abbr_dict.get(w, w) for w in doc_punctuation.split()])) \n",
    "    lemmatized_sentence = lemmatizer(doc_corrected)\n",
    "    result = remove_stopword(lemmatized_sentence)\n",
    "    cln.append(result)\n",
    "clean = pd.DataFrame(data=np.array(cln), columns= ['CommentClean'])\n",
    "\n",
    "#-----------------------------------------------Classificação---------------------------------------------------#\n",
    "\n",
    "def pattern1(pru, tags1, tags2, h3):\n",
    "    functionality = ''\n",
    "    j = 0\n",
    "    while (j < (len(pru)-3)):\n",
    "        if pru[j].pos_ in tags1 and pru[j+1].pos_ in tags2 and pru[j+2].pos_ == 'VERB' and pru[j+3].pos_ == 'NOUN':\n",
    "            func = [pru[j+2].text, pru[j+3].text]\n",
    "            functionality = ' '.join(func)\n",
    "            h3 = h3 + 1\n",
    "            return functionality, h3\n",
    "        else:\n",
    "            j = j + 1\n",
    "    return functionality, h3\n",
    "\n",
    "def pattern2(pru, tags3, tags4, h1):\n",
    "    functionality = ''\n",
    "    j = 0\n",
    "    while (j < (len(pru)-3)):\n",
    "        if pru[j].pos_ in tags3 and pru[j+1].pos_ in tags4 and pru[j+2].pos_ == 'VERB' and pru[j+3].pos_ == 'NOUN':\n",
    "            func = [pru[j+2].text, pru[j+3].text]\n",
    "            functionality = ' '.join(func)\n",
    "            h1 = h1+ 1\n",
    "            return functionality, h1\n",
    "        else:\n",
    "            j = j + 1\n",
    "    return functionality, h1\n",
    "\n",
    "def pattern3(pru, tags5, h1):\n",
    "    functionality = ''\n",
    "    j = 0\n",
    "    while (j < (len(pru)-3)):\n",
    "        if pru[j].pos_ in tags5 and pru[j+1].pos_ == 'VERB' and pru[j+2].pos_ == 'NOUN':\n",
    "            func = [pru[j+2].text, pru[j+3].text]\n",
    "            functionality = ' '.join(func)\n",
    "            h1 = h1 + 1\n",
    "            return functionality, h1\n",
    "        else:\n",
    "            j = j + 1\n",
    "    return functionality, h1\n",
    "\n",
    "def hypothesis2(pru):\n",
    "    j = 0\n",
    "    while (j < (len(pru)-3)):\n",
    "        if pru[j].text == 'não' and pru[j+1].text == 'conseguir':\n",
    "            return 1\n",
    "        else:\n",
    "            j = j + 1\n",
    "    return 0\n",
    "\n",
    "functionalities = [] \n",
    "\n",
    "tags1 = ['ADV']\n",
    "tags2 = ['VERB']\n",
    "tags3 = ['VERB', 'DET', 'PROPN', 'NOUN', 'PRON', 'ADP', 'ADV'] \n",
    "tags4 = ['VERB', 'PROPN', 'NOUN', 'ADV', 'AUX', 'ADJ']\n",
    "tags5 = ['ADJ', 'ADV', 'PROPN', 'NOUN', 'VERB']\n",
    "\n",
    "h1 = 0\n",
    "h2 = 0\n",
    "h3 = 0\n",
    "\n",
    "for i in range(len(cln)):\n",
    "    pru = nlp(clean['CommentClean'][i])\n",
    "    functionality = '' \n",
    "    if len(pru) > 3:\n",
    "        functionality, h3 = pattern1(pru, tags1, tags2, h3) \n",
    "        if len(functionality) == 0:\n",
    "            functionality, h1 = pattern2(pru, tags3, tags4, h1) \n",
    "        if len(functionality) == 0:\n",
    "            functionality, h1 = pattern3(pru, tags5, h1) \n",
    "        if len(functionality) == 0:\n",
    "            functionality = '-'\n",
    "    else:\n",
    "        functionality = '-'\n",
    "    h2 = h2 + (hypothesis2(pru))\n",
    "    functionalities.append(functionality)\n",
    "h1 = h1 + h3\n",
    "\n",
    "ser = pd.DataFrame(data=np.array(functionalities), index= range(len(data)), columns= ['Funcionalidade'])\n",
    "df = pd.concat([data, ser], axis=1)\n",
    "df.to_excel('Resultado.xlsx', index=False)\n",
    "\n",
    "print(\"Quantidades de PRUS pertencentes a hipótese 1: \" + str(h1))\n",
    "print(\"Quantidades de PRUS pertencentes a hipótese 2: \" + str(h2))\n",
    "print(\"Quantidades de PRUS pertencentes a hipótese 3: \" + str(h3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
