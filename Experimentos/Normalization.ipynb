{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         CommentClean\n",
      "0                                    não pego menores\n",
      "1               travar toda vez seguida mensagem erro\n",
      "2   vou cadastrar não abrir sempre fica dizer não ...\n",
      "3                             travar ver gráfico peso\n",
      "4                             não ser possível entrar\n",
      "..                                                ...\n",
      "94  acontecer comigo tempo matrícula passar aparec...\n",
      "95  não conseguir começar ontem hoje não conseguir...\n",
      "96                 não conseguir matricular oncologia\n",
      "97  excluir sim algum motivo sistema não permitir ...\n",
      "98  tentar fazer trancamento internet não encontra...\n",
      "\n",
      "[99 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.pt.examples import sentences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "data = pd.read_csv('prus (2).csv')\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "cln = []\n",
    "\n",
    "# Carregar dicionário de abreviaturas/gírias/erros\n",
    "def setup_abbr():\n",
    "\tfile = open(\"dic_portuguese.txt\", encoding='utf-8')\n",
    "\tabbr_dict = {}\n",
    "\n",
    "\tfor line in file:\n",
    "\t\tw = line.split(\";\")\n",
    "\t\tabbr_dict[w[0]] = w[1].replace(\"\\n\", \"\")\n",
    "\tfile.close()\n",
    "\n",
    "\treturn abbr_dict\n",
    "\n",
    "abbr_dict = setup_abbr()\n",
    "\n",
    "for i in range(len(data)):  \n",
    "\tdoc = nlp(data['comments'][i])\n",
    "\tlemma_sentence = []\n",
    "    \n",
    "#transformando a sentença em caixa baixa\n",
    "\tdoc_lower = doc.text.lower()\n",
    "\n",
    "#remoção de pontuações, números e caracteres especiais\n",
    "\tdoc_punctuation = re.sub('[^a-zãàáâéêíõóôúç \\n]', ' ', doc_lower)\n",
    "    \n",
    "#correção ortográfica\n",
    "\tdoc_corrected = nlp(\" \".join([abbr_dict.get(w, w) for w in doc_punctuation.split()]))\n",
    "    \n",
    "#lematização  \n",
    "\tfor token in doc_corrected:\n",
    "\t\tif token.pos_ == 'VERB':\n",
    "\t\t\tlemma = token.lemma_\n",
    "\t\t\tlemma_sentence.append(lemma)\n",
    "\t\telse:\n",
    "\t\t\tlemma_sentence.append(token.text)\n",
    "\tlemmatized_sentence = ' '.join(lemma_sentence)\n",
    "    \n",
    "#remoção das stop words\n",
    "\tstop_words = set(stopwords.words('portuguese')+ \\\n",
    "\t[\"{user}\", \"{url}\", \"<br/>\", \"myfitnesspal\", \"sigaa\", \"neste\"]) \n",
    "\tstop_words.remove(\"não\")\n",
    "\tstop_words.remove(\"sem\")\n",
    "\tword_tokens = word_tokenize(lemmatized_sentence)\n",
    "\tfiltered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "\tfiltered_sentence = [] \n",
    "\tfor w in word_tokens: \n",
    "\t\tif w not in stop_words: \n",
    "\t\t\tfiltered_sentence.append(w)\n",
    "\tresult = ' '.join(filtered_sentence)\n",
    "    \n",
    "\tcln.append(result)\n",
    "    \n",
    "ser = pd.DataFrame(data=np.array(cln), columns= ['CommentClean'])\n",
    "print(ser)\n",
    "#ser.to_csv('pruNormalized (2).csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
